#!/usr/bin/env python3
"""
é‡è¤‡ã—ãŸNotionãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œå‡ºãƒ»å‰Šé™¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import json
from pathlib import Path
from typing import Dict, List, Tuple
import hashlib


def calculate_file_hash(filepath: Path) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—"""
    hash_sha256 = hashlib.sha256()
    with open(filepath, 'rb') as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_sha256.update(chunk)
    return hash_sha256.hexdigest()


def find_duplicate_files(directory: Path) -> Dict[str, List[Path]]:
    """é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º"""
    file_hashes = {}
    
    for filepath in directory.glob("notion_*.txt"):
        if filepath.is_file():
            file_hash = calculate_file_hash(filepath)
            if file_hash not in file_hashes:
                file_hashes[file_hash] = []
            file_hashes[file_hash].append(filepath)
    
    # é‡è¤‡ãŒã‚ã‚‹ã‚‚ã®ã®ã¿è¿”ã™
    duplicates = {hash_val: files for hash_val, files in file_hashes.items() if len(files) > 1}
    return duplicates


def find_naming_pattern_duplicates(directory: Path) -> List[Tuple[Path, Path]]:
    """ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚‹é‡è¤‡ã‚’æ¤œå‡ºï¼ˆ_1.txtä»˜ãã¨ãªã—ï¼‰"""
    files = list(directory.glob("notion_*.txt"))
    duplicates = []
    
    for file in files:
        if file.name.endswith("_1.txt"):
            # _1.txtã‚’é™¤ã„ãŸåå‰ã§åŒåãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
            base_name = file.name[:-6] + ".txt"  # _1.txtã‚’.txtã«å¤‰æ›´
            base_file = directory / base_name
            if base_file.exists():
                duplicates.append((file, base_file))
    
    return duplicates


def clean_duplicates(directory: Path, dry_run: bool = True) -> Dict[str, int]:
    """é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    stats = {
        "content_duplicates": 0,
        "naming_duplicates": 0,
        "total_removed": 0
    }
    
    print(f"ğŸ” é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ä¸­: {directory}")
    
    # 1. å†…å®¹ã«ã‚ˆã‚‹é‡è¤‡ãƒã‚§ãƒƒã‚¯
    content_duplicates = find_duplicate_files(directory)
    if content_duplicates:
        print(f"\nğŸ“„ å†…å®¹ãŒåŒã˜ãƒ•ã‚¡ã‚¤ãƒ«: {len(content_duplicates)}ã‚°ãƒ«ãƒ¼ãƒ—")
        for i, (hash_val, files) in enumerate(content_duplicates.items(), 1):
            print(f"\n  ã‚°ãƒ«ãƒ¼ãƒ— {i} ({len(files)}ä»¶):")
            # æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ®‹ã—ã€æ®‹ã‚Šã‚’å‰Šé™¤å¯¾è±¡ã«ã™ã‚‹
            files_sorted = sorted(files, key=lambda x: x.name)
            keep_file = files_sorted[0]
            remove_files = files_sorted[1:]
            
            print(f"    âœ… ä¿æŒ: {keep_file.name}")
            for remove_file in remove_files:
                print(f"    âŒ å‰Šé™¤å¯¾è±¡: {remove_file.name}")
                if not dry_run:
                    remove_file.unlink()
                    print(f"       â†’ å‰Šé™¤å®Œäº†")
                stats["content_duplicates"] += 1
    
    # 2. ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚‹é‡è¤‡ãƒã‚§ãƒƒã‚¯
    naming_duplicates = find_naming_pattern_duplicates(directory)
    if naming_duplicates:
        print(f"\nğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚‹é‡è¤‡: {len(naming_duplicates)}ãƒšã‚¢")
        for i, (numbered_file, base_file) in enumerate(naming_duplicates, 1):
            print(f"\n  ãƒšã‚¢ {i}:")
            print(f"    ğŸ“„ {base_file.name} ({base_file.stat().st_size} bytes)")
            print(f"    ğŸ“„ {numbered_file.name} ({numbered_file.stat().st_size} bytes)")
            
            # å†…å®¹ã‚’æ¯”è¼ƒ
            if base_file.read_text(encoding='utf-8') == numbered_file.read_text(encoding='utf-8'):
                print(f"    âœ… å†…å®¹ãŒåŒã˜ â†’ {numbered_file.name}ã‚’å‰Šé™¤")
                if not dry_run:
                    numbered_file.unlink()
                    print(f"       â†’ å‰Šé™¤å®Œäº†")
                stats["naming_duplicates"] += 1
            else:
                print(f"    âš ï¸  å†…å®¹ãŒç•°ãªã‚‹ â†’ æ‰‹å‹•ç¢ºèªãŒå¿…è¦")
    
    stats["total_removed"] = stats["content_duplicates"] + stats["naming_duplicates"]
    return stats


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    notion_dir = Path("data/sources/notion")
    
    if not notion_dir.exists():
        print(f"âŒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {notion_dir}")
        return 1
    
    print("=" * 60)
    print("ğŸ§¹ Notionè­°äº‹éŒ²é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ« ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ„ãƒ¼ãƒ«")
    print("=" * 60)
    
    # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ã§é‡è¤‡ã‚’ç¢ºèª
    print("\nã€ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ã€‘é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œå‡º...")
    stats_dry = clean_duplicates(notion_dir, dry_run=True)
    
    if stats_dry["total_removed"] == 0:
        print("\nâœ… é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return 0
    
    print(f"\nğŸ“Š é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ:")
    print(f"  - å†…å®¹ã«ã‚ˆã‚‹é‡è¤‡: {stats_dry['content_duplicates']}ä»¶")
    print(f"  - å‘½åã«ã‚ˆã‚‹é‡è¤‡: {stats_dry['naming_duplicates']}ä»¶")
    print(f"  - å‰Šé™¤å¯¾è±¡åˆè¨ˆ: {stats_dry['total_removed']}ä»¶")
    
    # å®Ÿè¡Œç¢ºèª
    print(f"\nğŸ”§ {stats_dry['total_removed']}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ")
    response = input("å®Ÿè¡Œã™ã‚‹å ´åˆã¯ 'yes' ã‚’å…¥åŠ›: ").strip().lower()
    
    if response == 'yes':
        print("\nğŸ—‘ï¸  é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ä¸­...")
        stats_real = clean_duplicates(notion_dir, dry_run=False)
        
        print(f"\nâœ… ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†!")
        print(f"  - å‰Šé™¤ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«: {stats_real['total_removed']}ä»¶")
        
        # æ®‹å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ç¢ºèª
        remaining_files = len(list(notion_dir.glob("notion_*.txt")))
        print(f"  - æ®‹å­˜ãƒ•ã‚¡ã‚¤ãƒ«: {remaining_files}ä»¶")
        
    else:
        print("\nâŒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = main()
    exit(exit_code) 