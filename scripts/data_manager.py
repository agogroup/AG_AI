#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ç®¡ç†ãƒ˜ãƒ«ãƒ‘ãƒ¼
æ–°è¦ãƒ»å‡¦ç†æ¸ˆã¿ãƒ»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã®è‡ªå‹•ç®¡ç†
"""
import os
import shutil
import json
import hashlib
from datetime import datetime, timedelta

# date_utilsã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆç›¸å¯¾/çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®ä¸¡æ–¹ã«å¯¾å¿œï¼‰
try:
    from .date_utils import get_today, get_now
except ImportError:
    from date_utils import get_today, get_now
from pathlib import Path
from typing import List, Tuple, Optional


class DataManager:
    """ãƒ‡ãƒ¼ã‚¿ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, base_dir: str = "data"):
        self.base_dir = Path(base_dir)
        self.new_dir = self.base_dir / "00_new"
        self.analyzed_dir = self.base_dir / "01_analyzed"
        self.archive_dir = self.base_dir / "02_archive"
        self.log_file = self.base_dir / "analysis_log.json"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        for dir_path in [self.new_dir, self.analyzed_dir, self.archive_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
            
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åˆæœŸåŒ–
        if not self.log_file.exists():
            self._init_log_file()
    
    def _init_log_file(self):
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–"""
        initial_log = {
            "processed_files": [],
            "last_cleanup": get_now(),
            "statistics": {
                "total_processed": 0,
                "total_archived": 0
            }
        }
        with open(self.log_file, 'w', encoding='utf-8') as f:
            json.dump(initial_log, f, ensure_ascii=False, indent=2)
    
    def calculate_file_hash(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def get_new_files(self) -> List[Path]:
        """æœªå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        files = []
        for ext in ['*.txt', '*.json', '*.csv', '*.log']:
            files.extend(self.new_dir.glob(ext))
        return sorted(files)
    
    def check_duplicate(self, file_path: Path) -> Tuple[bool, Optional[str]]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯"""
        file_hash = self.calculate_file_hash(file_path)
        
        with open(self.log_file, 'r', encoding='utf-8') as f:
            log = json.load(f)
        
        for record in log['processed_files']:
            if record.get('file_hash') == file_hash:
                return True, record.get('processed_date')
        
        return False, None
    
    def move_to_analyzed(self, file_path: Path, analysis_result_path: Optional[str] = None):
        """å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ã«ç§»å‹•"""
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        is_duplicate, processed_date = self.check_duplicate(file_path)
        if is_duplicate:
            print(f"âš ï¸  æ—¢ã«å‡¦ç†æ¸ˆã¿ã§ã™: {file_path.name} (å‡¦ç†æ—¥: {processed_date})")
            return False
        
        # ç§»å‹•å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        today = get_today()
        dest_dir = self.analyzed_dir / today
        dest_dir.mkdir(exist_ok=True)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•
        dest_path = dest_dir / file_path.name
        if dest_path.exists():
            # åŒåãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ä»˜åŠ 
            timestamp = datetime.now().strftime("%H%M%S")
            dest_path = dest_dir / f"{file_path.stem}_{timestamp}{file_path.suffix}"
        
        shutil.move(str(file_path), str(dest_path))
        
        # ãƒ­ã‚°æ›´æ–°
        self._update_log(file_path, dest_path, analysis_result_path)
        
        print(f"âœ… å‡¦ç†å®Œäº†: {file_path.name} â†’ {dest_path.relative_to(self.base_dir)}")
        return True
    
    def _update_log(self, original_path: Path, dest_path: Path, analysis_result_path: Optional[str]):
        """å‡¦ç†å±¥æ­´ã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        with open(self.log_file, 'r', encoding='utf-8') as f:
            log = json.load(f)
        
        # æ–°ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ä½œæˆ
        record = {
            "filename": original_path.name,
            "original_path": str(original_path),
            "processed_date": get_now(),
            "moved_to": str(dest_path),
            "analysis_results": analysis_result_path,
            "file_hash": self.calculate_file_hash(dest_path),
            "file_size": dest_path.stat().st_size,
            "status": "completed"
        }
        
        log['processed_files'].append(record)
        log['statistics']['total_processed'] += 1
        
        # ãƒ­ã‚°ä¿å­˜
        with open(self.log_file, 'w', encoding='utf-8') as f:
            json.dump(log, f, ensure_ascii=False, indent=2)
    
    def archive_old_files(self, days: int = 180):
        """æŒ‡å®šæ—¥æ•°ä»¥ä¸Šå‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–"""
        cutoff_date = datetime.now() - timedelta(days=days)
        archived_count = 0
        
        for date_dir in self.analyzed_dir.iterdir():
            if date_dir.is_dir() and '-' in date_dir.name:
                try:
                    dir_date = datetime.strptime(date_dir.name, "%Y-%m-%d")
                    if dir_date < cutoff_date:
                        dest = self.archive_dir / date_dir.name
                        shutil.move(str(date_dir), str(dest))
                        archived_count += 1
                        print(f"ğŸ“¦ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–: {date_dir.name}")
                except ValueError:
                    # æ—¥ä»˜å½¢å¼ã§ãªã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ã‚¹ã‚­ãƒƒãƒ—
                    continue
        
        if archived_count > 0:
            # ãƒ­ã‚°æ›´æ–°
            with open(self.log_file, 'r', encoding='utf-8') as f:
                log = json.load(f)
            log['statistics']['total_archived'] += archived_count
            log['last_cleanup'] = get_now()
            with open(self.log_file, 'w', encoding='utf-8') as f:
                json.dump(log, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… {archived_count}å€‹ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã—ã¾ã—ãŸ")
    
    def get_statistics(self):
        """å‡¦ç†çµ±è¨ˆã‚’è¡¨ç¤º"""
        with open(self.log_file, 'r', encoding='utf-8') as f:
            log = json.load(f)
        
        stats = log['statistics']
        print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿å‡¦ç†çµ±è¨ˆ")
        print("=" * 40)
        print(f"ç·å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['total_processed']}")
        print(f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ¸ˆã¿: {stats['total_archived']}")
        print(f"æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: {log['last_cleanup'][:10]}")
        
        # ç¾åœ¨ã®çŠ¶æ³
        new_files = len(self.get_new_files())
        analyzed_dirs = len(list(self.analyzed_dir.iterdir()))
        archive_dirs = len(list(self.archive_dir.iterdir()))
        
        print(f"\nç¾åœ¨ã®çŠ¶æ³:")
        print(f"  æœªå‡¦ç†: {new_files}ãƒ•ã‚¡ã‚¤ãƒ«")
        print(f"  å‡¦ç†æ¸ˆã¿: {analyzed_dirs}æ—¥åˆ†")
        print(f"  ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–: {archive_dirs}æ—¥åˆ†")
    
    def cleanup_duplicates(self):
        """é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        with open(self.log_file, 'r', encoding='utf-8') as f:
            log = json.load(f)
        
        # ãƒãƒƒã‚·ãƒ¥å€¤ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        hash_map = {}
        for record in log['processed_files']:
            file_hash = record.get('file_hash')
            if file_hash:
                if file_hash not in hash_map:
                    hash_map[file_hash] = []
                hash_map[file_hash].append(record)
        
        # é‡è¤‡ã‚’æ¤œå‡º
        duplicates_found = 0
        for file_hash, records in hash_map.items():
            if len(records) > 1:
                print(f"\nâš ï¸  é‡è¤‡æ¤œå‡º: {records[0]['filename']}")
                for record in records[1:]:
                    print(f"  - {record['processed_date']}: {record['moved_to']}")
                duplicates_found += 1
        
        if duplicates_found == 0:
            print("âœ… é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import sys
    
    dm = DataManager()
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "status":
            dm.get_statistics()
        
        elif command == "archive":
            days = int(sys.argv[2]) if len(sys.argv) > 2 else 180
            dm.archive_old_files(days)
        
        elif command == "cleanup":
            dm.cleanup_duplicates()
        
        elif command == "list":
            files = dm.get_new_files()
            if files:
                print(f"\nğŸ“ æœªå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ« ({len(files)}å€‹):")
                for f in files:
                    print(f"  - {f.name}")
            else:
                print("âœ… æœªå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“")
        
        else:
            print(f"âŒ ä¸æ˜ãªã‚³ãƒãƒ³ãƒ‰: {command}")
            print("\nä½¿ç”¨æ–¹æ³•:")
            print("  python data_manager.py status    # çµ±è¨ˆè¡¨ç¤º")
            print("  python data_manager.py list      # æœªå‡¦ç†ä¸€è¦§")
            print("  python data_manager.py archive   # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å®Ÿè¡Œ")
            print("  python data_manager.py cleanup   # é‡è¤‡ãƒã‚§ãƒƒã‚¯")
    
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯çµ±è¨ˆè¡¨ç¤º
        dm.get_statistics()


if __name__ == "__main__":
    main()