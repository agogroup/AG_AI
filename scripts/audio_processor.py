#!/usr/bin/env python3
"""
éŸ³å£°å‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
Whisperã‚’ä½¿ç”¨ã—ãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®æ–‡å­—èµ·ã“ã—æ©Ÿèƒ½
"""
import os
import sys
import time
import json
import torch
import warnings
warnings.filterwarnings("ignore")

from pathlib import Path
from typing import Dict, Optional, Tuple
from datetime import datetime

try:
    import whisper
except ImportError:
    print("âš ï¸  whisperãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("    pip install openai-whisper")
    sys.exit(1)

# date_utilsã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from .date_utils import get_now
except ImportError:
    from date_utils import get_now


class AudioProcessor:
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_size: str = "turbo", device: Optional[str] = None):
        """
        åˆæœŸåŒ–
        
        Args:
            model_size: Whisperãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºï¼ˆtiny, base, small, medium, large, turboï¼‰
            device: ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹ï¼ˆNone, cuda, cpu, mpsï¼‰
            
        æ³¨æ„:
            ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯'turbo'ãƒ¢ãƒ‡ãƒ«ï¼ˆLarge V3 Turbo - é«˜ç²¾åº¦ã§6å€é«˜é€Ÿï¼‰
            MPSãƒ‡ãƒã‚¤ã‚¹å¯¾å¿œï¼ˆM1/M2 Mac GPUæœ€é©åŒ–ï¼‰
        """
        self.model_size = model_size
        
        # ãƒ‡ãƒã‚¤ã‚¹ã®è‡ªå‹•é¸æŠã¨MPSå¯¾å¿œï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ä»˜ãï¼‰
        if device is None:
            # MPSãŒåˆ©ç”¨å¯èƒ½ã‹è©¦è¡Œã€å¤±æ•—æ™‚ã¯CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if torch.backends.mps.is_available():
                self.device = "mps"  # M1/M2 Mac GPUæœ€é©åŒ–ï¼ˆå®Ÿéš›ã®ãƒ†ã‚¹ãƒˆã¯å¾Œã§å®Ÿè¡Œï¼‰
            elif torch.cuda.is_available():
                self.device = "cuda"
            else:
                self.device = "cpu"
        else:
            self.device = device
            
        print(f"ğŸ”§ Whisperãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­... (ãƒ¢ãƒ‡ãƒ«: {model_size}, ãƒ‡ãƒã‚¤ã‚¹: {self.device})")
        
        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒã‚¤ã‚¹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œï¼‰
        try:
            self.model = whisper.load_model(model_size, device=self.device)
            print(f"âœ… Whisperãƒ¢ãƒ‡ãƒ«ï¼ˆ{model_size}ï¼‰ã®ãƒ­ãƒ¼ãƒ‰å®Œäº† - ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        except Exception as e:
            # MPSã§å¤±æ•—ã—ãŸå ´åˆã€CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if self.device == "mps":
                print(f"âš ï¸  MPSä½¿ç”¨ä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã€CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¸­...")
                try:
                    self.device = "cpu"
                    self.model = whisper.load_model(model_size, device=self.device)
                    print(f"âœ… Whisperãƒ¢ãƒ‡ãƒ«ï¼ˆ{model_size}ï¼‰ã®ãƒ­ãƒ¼ãƒ‰å®Œäº† - ãƒ‡ãƒã‚¤ã‚¹: {self.device} (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)")
                except Exception as fallback_error:
                    print(f"âŒ CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å¤±æ•—: {fallback_error}")
                    raise
            else:
                print(f"âŒ ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                raise
    
    def transcribe_audio(self, audio_path: Path, language: str = "ja") -> Dict:
        """
        éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—
        
        Args:
            audio_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            language: è¨€èªã‚³ãƒ¼ãƒ‰ï¼ˆja=æ—¥æœ¬èª, en=è‹±èª, auto=è‡ªå‹•æ¤œå‡ºï¼‰
        
        Returns:
            æ–‡å­—èµ·ã“ã—çµæœã®è¾æ›¸
        """
        start_time = time.time()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
        file_size_mb = audio_path.stat().st_size / (1024 * 1024)
        print(f"\nğŸµ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {audio_path.name}")
        print(f"   ã‚µã‚¤ã‚º: {file_size_mb:.1f} MB")
        
        try:
            # æœ€é©åŒ–ã•ã‚ŒãŸæ–‡å­—èµ·ã“ã—è¨­å®š
            options = {
                "beam_size": 2,           # ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã§ç²¾åº¦å‘ä¸Š
                "temperature": 0.0,       # ç¢ºå®šçš„ãƒ‡ã‚³ãƒ¼ãƒ‰
                "best_of": 1,            # é«˜é€ŸåŒ–ã®ãŸã‚1å›ã®ã¿
                "patience": 1.0          # ãƒ‡ã‚³ãƒ¼ãƒ‰å¿è€åº¦
            }
            
            # è¨€èªå›ºå®šè¨­å®šï¼ˆç²¾åº¦å¤§å¹…å‘ä¸Šï¼‰
            if language != "auto":
                options["language"] = language
            
            # æ–‡å­—èµ·ã“ã—å®Ÿè¡Œï¼ˆæœ€é©åŒ–è¨­å®šï¼‰
            print(f"   ğŸš€ æœ€é©åŒ–æ–‡å­—èµ·ã“ã—ä¸­... ({self.model_size}ãƒ¢ãƒ‡ãƒ« + ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒ)")
            result = self.model.transcribe(str(audio_path), **options)
            
            # å‡¦ç†æ™‚é–“è¨ˆç®—
            processing_time = time.time() - start_time
            
            # éŸ³å£°ã®é•·ã•ã‚’æ¨å®šï¼ˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‹ã‚‰ï¼‰
            if result.get("segments"):
                audio_duration = result["segments"][-1]["end"]
            else:
                audio_duration = 0
            
            # çµæœã‚’ã¾ã¨ã‚ã‚‹
            transcription_result = {
                "text": result["text"],
                "language": result.get("language", language),
                "segments": result.get("segments", []),
                "metadata": {
                    "audio_file": audio_path.name,
                    "audio_size_mb": round(file_size_mb, 2),
                    "audio_duration_sec": round(audio_duration, 2),
                    "processing_time_sec": round(processing_time, 2),
                    "transcription_date": get_now(),
                    "model_used": self.model_size,
                    "device": self.device,
                    "char_count": len(result["text"]),
                    "detected_language": result.get("language", "unknown")
                }
            }
            
            # å‡¦ç†å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            duration_min = int(audio_duration // 60)
            duration_sec = int(audio_duration % 60)
            print(f"âœ… æ–‡å­—èµ·ã“ã—å®Œäº† ({duration_min}åˆ†{duration_sec}ç§’ â†’ {len(result['text'])}æ–‡å­—)")
            print(f"   å‡¦ç†æ™‚é–“: {processing_time:.1f}ç§’")
            
            return transcription_result
            
        except Exception as e:
            print(f"âŒ æ–‡å­—èµ·ã“ã—ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def save_transcription(self, result: Dict, output_dir: Path, base_filename: str):
        """
        æ–‡å­—èµ·ã“ã—çµæœã‚’ä¿å­˜
        
        Args:
            result: æ–‡å­—èµ·ã“ã—çµæœ
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            base_filename: ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰
        """
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        text_path = output_dir / f"{base_filename}_transcription.txt"
        with open(text_path, 'w', encoding='utf-8') as f:
            # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±
            f.write(f"ã€éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«æ–‡å­—èµ·ã“ã—ã€‘\n")
            f.write(f"ãƒ•ã‚¡ã‚¤ãƒ«å: {result['metadata']['audio_file']}\n")
            f.write(f"éŸ³å£°é•·ã•: {result['metadata']['audio_duration_sec']}ç§’\n")
            f.write(f"æ–‡å­—èµ·ã“ã—æ—¥æ™‚: {result['metadata']['transcription_date']}\n")
            f.write(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: Whisper {result['metadata']['model_used']}\n")
            f.write(f"æ¤œå‡ºè¨€èª: {result['metadata']['detected_language']}\n")
            f.write("=" * 60 + "\n\n")
            
            # æœ¬æ–‡
            f.write(result['text'])
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼ˆè©³ç´°æƒ…å ±ä»˜ãï¼‰
        json_path = output_dir / f"{base_filename}_transcription.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ ä¿å­˜å®Œäº†:")
        print(f"   - ãƒ†ã‚­ã‚¹ãƒˆ: {text_path.relative_to(output_dir.parent.parent)}")
        print(f"   - è©³ç´°æƒ…å ±: {json_path.relative_to(output_dir.parent.parent)}")
        
        return text_path, json_path
    
    def estimate_processing_time(self, audio_path: Path) -> float:
        """
        å‡¦ç†æ™‚é–“ã®æ¨å®š
        
        Args:
            audio_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            æ¨å®šå‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰
        """
        file_size_mb = audio_path.stat().st_size / (1024 * 1024)
        
        # ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹å‡¦ç†é€Ÿåº¦ã®ç›®å®‰ï¼ˆMB/ç§’ï¼‰
        # å®Ÿæ¸¬å€¤ã«åŸºã¥ãæ¨å®šï¼ˆ2025å¹´6æœˆå®Ÿç¸¾ + Turboå¯¾å¿œï¼‰
        speed_estimates = {
            "tiny": 8.0,     # å®Ÿæ¸¬: é«˜é€Ÿã ãŒç²¾åº¦ã‚„ã‚„åŠ£ã‚‹
            "base": 4.0,     # å®Ÿæ¸¬: é€Ÿåº¦ã¨ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹æœ€é©
            "small": 2.0,    # å®Ÿæ¸¬: é«˜ç²¾åº¦ã€ã‚„ã‚„é‡ã„
            "medium": 0.8,   # å®Ÿæ¸¬: 2åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒªã‚¹ã‚¯
            "large": 0.4,    # å®Ÿæ¸¬: éå¸¸ã«é‡ã„
            "turbo": 24.0    # æ–°å®Ÿè£…: Large V2ç²¾åº¦ã§6å€é«˜é€Ÿ
        }
        
        speed = speed_estimates.get(self.model_size, 1.0)
        
        # GPUæœ€é©åŒ–ã«ã‚ˆã‚‹ã•ã‚‰ãªã‚‹é«˜é€ŸåŒ–
        if self.device == "mps":       # M1/M2 Mac GPU
            speed *= 2.5
        elif self.device == "cuda":    # NVIDIA GPU
            speed *= 3
            
        return file_size_mb / speed
    
    def check_audio_quality(self, audio_path: Path) -> str:
        """
        éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å“è³ªã‚’æ¨å®šã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’æ¨å¥¨
        
        Args:
            audio_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º
        """
        file_size_mb = audio_path.stat().st_size / (1024 * 1024)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å“è³ªã‚’æ¨å®š
        filename_lower = audio_path.name.lower()
        
        # ä¼šè­°éŒ²éŸ³ã®å ´åˆ
        if any(keyword in filename_lower for keyword in ['ä¼šè­°', 'meeting', 'zoom', 'teams', 'éŒ²éŸ³']):
            return "medium"  # ãƒã‚¤ã‚ºãŒå¤šã„å¯èƒ½æ€§
        
        # ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã®å ´åˆ
        if any(keyword in filename_lower for keyword in ['interview', 'ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼', 'å¯¾è«‡']):
            return "small"  # æ¯”è¼ƒçš„ã‚¯ãƒªã‚¢
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‹ã‚‰æ¨å®šï¼ˆClaude Codeç’°å¢ƒæœ€é©åŒ–ï¼‰
        if file_size_mb > 50:   # å¤§ãã„ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ50MBè¶…ï¼‰
            return "tiny"       # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿å„ªå…ˆ
        elif file_size_mb > 20: # ä¸­ç¨‹åº¦ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ20-50MBï¼‰
            return "base"       # ãƒãƒ©ãƒ³ã‚¹é‡è¦–
        elif file_size_mb < 5:  # å°ã•ã„ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ5MBæœªæº€ï¼‰
            return "small"      # é«˜ç²¾åº¦å¯èƒ½
        else:                   # æ¨™æº–ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ5-20MBï¼‰
            return "base"       # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¨å¥¨


def process_audio_file(audio_path: Path, output_dir: Path, 
                      model_size: Optional[str] = None,
                      language: str = "ja") -> Tuple[Path, Dict]:
    """
    éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
    
    Args:
        audio_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        model_size: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºï¼ˆNoneã®å ´åˆã¯è‡ªå‹•é¸æŠï¼‰
        language: è¨€èªã‚³ãƒ¼ãƒ‰
        
    Returns:
        (ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹, æ–‡å­—èµ·ã“ã—çµæœ)
    """
    # ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®è‡ªå‹•é¸æŠ
    if model_size is None:
        processor_temp = AudioProcessor(model_size="tiny")  # å“è³ªãƒã‚§ãƒƒã‚¯ç”¨
        model_size = processor_temp.check_audio_quality(audio_path)
        print(f"ğŸ¤– è‡ªå‹•é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«: {model_size}")
    
    # éŸ³å£°å‡¦ç†å®Ÿè¡Œ
    processor = AudioProcessor(model_size=model_size)
    
    # å‡¦ç†æ™‚é–“æ¨å®š
    estimated_time = processor.estimate_processing_time(audio_path)
    if estimated_time > 60:
        print(f"â±ï¸  æ¨å®šå‡¦ç†æ™‚é–“: {int(estimated_time // 60)}åˆ†{int(estimated_time % 60)}ç§’")
    else:
        print(f"â±ï¸  æ¨å®šå‡¦ç†æ™‚é–“: {int(estimated_time)}ç§’")
    
    # æ–‡å­—èµ·ã“ã—å®Ÿè¡Œ
    result = processor.transcribe_audio(audio_path, language=language)
    
    # çµæœä¿å­˜
    base_filename = audio_path.stem
    text_path, json_path = processor.save_transcription(result, output_dir, base_filename)
    
    return text_path, result


def main():
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python audio_processor.py <éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«>")
        print("ã‚ªãƒ—ã‚·ãƒ§ãƒ³:")
        print("  --model <size>    ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º (tiny/base/small/medium/large)")
        print("  --lang <code>     è¨€èªã‚³ãƒ¼ãƒ‰ (ja/en/auto)")
        sys.exit(1)
    
    audio_file = Path(sys.argv[1])
    if not audio_file.exists():
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {audio_file}")
        sys.exit(1)
    
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³è§£æ
    model_size = None
    language = "ja"
    
    for i, arg in enumerate(sys.argv):
        if arg == "--model" and i + 1 < len(sys.argv):
            model_size = sys.argv[i + 1]
        elif arg == "--lang" and i + 1 < len(sys.argv):
            language = sys.argv[i + 1]
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    output_dir = Path("data/01_analyzed") / datetime.now().strftime("%Y-%m-%d")
    
    # å‡¦ç†å®Ÿè¡Œ
    try:
        text_path, result = process_audio_file(
            audio_file, output_dir, model_size, language
        )
        print(f"\nâœ¨ å‡¦ç†å®Œäº†ï¼")
        print(f"æ–‡å­—æ•°: {result['metadata']['char_count']}")
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()